{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea0551b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import necessary libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,precision_recall_curve,auc,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cad47dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "df_X = pd.read_csv('sequence-bs-features.txt', delimiter='\\s+', header=0) \n",
    "df_Y = pd.read_csv('gene-gomatrix.txt', delimiter='\\s+', header=0)\n",
    "predicted = pd.read_csv('final-predictions-human copy.txt', delimiter='\\s+', header=0, on_bad_lines = 'skip')\n",
    "predicted.index = predicted.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d94acd42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q3dna</th>\n",
       "      <th>q3carb</th>\n",
       "      <th>q3atp</th>\n",
       "      <th>q3ppi</th>\n",
       "      <th>q3rna</th>\n",
       "      <th>top5dna</th>\n",
       "      <th>top5carb</th>\n",
       "      <th>top5atp</th>\n",
       "      <th>top5ppi</th>\n",
       "      <th>top5rna</th>\n",
       "      <th>...</th>\n",
       "      <th>countN</th>\n",
       "      <th>countP</th>\n",
       "      <th>countQ</th>\n",
       "      <th>countR</th>\n",
       "      <th>countS</th>\n",
       "      <th>countT</th>\n",
       "      <th>countV</th>\n",
       "      <th>countW</th>\n",
       "      <th>countY</th>\n",
       "      <th>sqsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>P31946</th>\n",
       "      <td>0.310625</td>\n",
       "      <td>0.185669</td>\n",
       "      <td>0.103394</td>\n",
       "      <td>0.346282</td>\n",
       "      <td>0.015200</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>0.625905</td>\n",
       "      <td>0.176510</td>\n",
       "      <td>0.503740</td>\n",
       "      <td>0.070297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.060976</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.044715</td>\n",
       "      <td>2.390935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P62258</th>\n",
       "      <td>0.311250</td>\n",
       "      <td>0.168360</td>\n",
       "      <td>0.114797</td>\n",
       "      <td>0.349137</td>\n",
       "      <td>0.016576</td>\n",
       "      <td>0.8475</td>\n",
       "      <td>0.649310</td>\n",
       "      <td>0.156503</td>\n",
       "      <td>0.493863</td>\n",
       "      <td>0.074668</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043137</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.035294</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.054902</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>0.047059</td>\n",
       "      <td>2.406540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Q04917</th>\n",
       "      <td>0.459375</td>\n",
       "      <td>0.155227</td>\n",
       "      <td>0.129250</td>\n",
       "      <td>0.335464</td>\n",
       "      <td>0.018188</td>\n",
       "      <td>0.8805</td>\n",
       "      <td>0.660605</td>\n",
       "      <td>0.160915</td>\n",
       "      <td>0.457920</td>\n",
       "      <td>0.091139</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056911</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.052846</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>2.390935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P61981</th>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.189556</td>\n",
       "      <td>0.113903</td>\n",
       "      <td>0.345894</td>\n",
       "      <td>0.016431</td>\n",
       "      <td>0.8315</td>\n",
       "      <td>0.564121</td>\n",
       "      <td>0.175618</td>\n",
       "      <td>0.482345</td>\n",
       "      <td>0.076959</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>0.016194</td>\n",
       "      <td>0.048583</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.072874</td>\n",
       "      <td>0.044534</td>\n",
       "      <td>0.060729</td>\n",
       "      <td>0.008097</td>\n",
       "      <td>0.056680</td>\n",
       "      <td>2.392697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P31947</th>\n",
       "      <td>0.501250</td>\n",
       "      <td>0.113197</td>\n",
       "      <td>0.144869</td>\n",
       "      <td>0.353664</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>0.9150</td>\n",
       "      <td>0.618908</td>\n",
       "      <td>0.169927</td>\n",
       "      <td>0.454528</td>\n",
       "      <td>0.068090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.036290</td>\n",
       "      <td>0.052419</td>\n",
       "      <td>0.076613</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.040323</td>\n",
       "      <td>2.394452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           q3dna    q3carb     q3atp     q3ppi     q3rna  top5dna  top5carb  \\\n",
       "P31946  0.310625  0.185669  0.103394  0.346282  0.015200   0.7790  0.625905   \n",
       "P62258  0.311250  0.168360  0.114797  0.349137  0.016576   0.8475  0.649310   \n",
       "Q04917  0.459375  0.155227  0.129250  0.335464  0.018188   0.8805  0.660605   \n",
       "P61981  0.320000  0.189556  0.113903  0.345894  0.016431   0.8315  0.564121   \n",
       "P31947  0.501250  0.113197  0.144869  0.353664  0.017850   0.9150  0.618908   \n",
       "\n",
       "         top5atp   top5ppi   top5rna  ...    countN    countP    countQ  \\\n",
       "P31946  0.176510  0.503740  0.070297  ...  0.056911  0.020325  0.060976   \n",
       "P62258  0.156503  0.493863  0.074668  ...  0.043137  0.019608  0.035294   \n",
       "Q04917  0.160915  0.457920  0.091139  ...  0.056911  0.016260  0.052846   \n",
       "P61981  0.175618  0.482345  0.076959  ...  0.056680  0.016194  0.048583   \n",
       "P31947  0.169927  0.454528  0.068090  ...  0.032258  0.028226  0.036290   \n",
       "\n",
       "          countR    countS    countT    countV    countW    countY    sqsize  \n",
       "P31946  0.040650  0.077236  0.048780  0.044715  0.008130  0.044715  2.390935  \n",
       "P62258  0.054902  0.054902  0.039216  0.050980  0.007843  0.047059  2.406540  \n",
       "Q04917  0.048780  0.069106  0.032520  0.052846  0.008130  0.048780  2.390935  \n",
       "P61981  0.052632  0.072874  0.044534  0.060729  0.008097  0.056680  2.392697  \n",
       "P31947  0.052419  0.076613  0.040323  0.048387  0.008065  0.040323  2.394452  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1263b81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GO:0003674</th>\n",
       "      <th>GO:0005576</th>\n",
       "      <th>GO:0005615</th>\n",
       "      <th>GO:0008150</th>\n",
       "      <th>GO:0070062</th>\n",
       "      <th>GO:0000166</th>\n",
       "      <th>GO:0003676</th>\n",
       "      <th>GO:0003723</th>\n",
       "      <th>GO:0005515</th>\n",
       "      <th>GO:0005654</th>\n",
       "      <th>...</th>\n",
       "      <th>GO:0045892</th>\n",
       "      <th>GO:0003924</th>\n",
       "      <th>GO:0000122</th>\n",
       "      <th>GO:0004871</th>\n",
       "      <th>GO:0043565</th>\n",
       "      <th>GO:0006367</th>\n",
       "      <th>GO:0016567</th>\n",
       "      <th>GO:0004842</th>\n",
       "      <th>GO:0034220</th>\n",
       "      <th>GO:0005874</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1BG</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1CF</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2BP1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2LD1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2M</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 138 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GO:0003674  GO:0005576  GO:0005615  GO:0008150  GO:0070062  GO:0000166  \\\n",
       "A1BG          1.0         1.0         1.0         1.0         1.0         0.0   \n",
       "A1CF          0.0         0.0         0.0         0.0         0.0         1.0   \n",
       "A2BP1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A2LD1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A2M           0.0         1.0         0.0         0.0         1.0         0.0   \n",
       "\n",
       "       GO:0003676  GO:0003723  GO:0005515  GO:0005654  ...  GO:0045892  \\\n",
       "A1BG          0.0         0.0         0.0         0.0  ...         0.0   \n",
       "A1CF          1.0         1.0         1.0         1.0  ...         0.0   \n",
       "A2BP1         0.0         0.0         0.0         0.0  ...         0.0   \n",
       "A2LD1         0.0         0.0         0.0         0.0  ...         0.0   \n",
       "A2M           0.0         0.0         1.0         0.0  ...         0.0   \n",
       "\n",
       "       GO:0003924  GO:0000122  GO:0004871  GO:0043565  GO:0006367  GO:0016567  \\\n",
       "A1BG          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A1CF          0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A2BP1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A2LD1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "A2M           0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "       GO:0004842  GO:0034220  GO:0005874  \n",
       "A1BG          0.0         0.0         0.0  \n",
       "A1CF          0.0         0.0         0.0  \n",
       "A2BP1         0.0         0.0         0.0  \n",
       "A2LD1         0.0         0.0         0.0  \n",
       "A2M           0.0         0.0         0.0  \n",
       "\n",
       "[5 rows x 138 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d369df4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Protein</th>\n",
       "      <th>pred.seq</th>\n",
       "      <th>pred.ge</th>\n",
       "      <th>cons1</th>\n",
       "      <th>cons2</th>\n",
       "      <th>desall.g</th>\n",
       "      <th>desall.p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXA2</td>\n",
       "      <td>Q9Y261</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>0.122593</td>\n",
       "      <td>6.342503</td>\n",
       "      <td>0.517361</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXL2</td>\n",
       "      <td>P58012</td>\n",
       "      <td>0.491874</td>\n",
       "      <td>0.098542</td>\n",
       "      <td>4.847039</td>\n",
       "      <td>0.491874</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOXA13</td>\n",
       "      <td>P31271</td>\n",
       "      <td>0.483384</td>\n",
       "      <td>0.191965</td>\n",
       "      <td>9.279254</td>\n",
       "      <td>0.483384</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOXA3</td>\n",
       "      <td>O43365</td>\n",
       "      <td>0.461362</td>\n",
       "      <td>0.149816</td>\n",
       "      <td>6.911944</td>\n",
       "      <td>0.461362</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FOXA1</td>\n",
       "      <td>P55317</td>\n",
       "      <td>0.457259</td>\n",
       "      <td>0.052730</td>\n",
       "      <td>2.411128</td>\n",
       "      <td>0.457259</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene Protein  pred.seq   pred.ge     cons1     cons2  desall.g  desall.p\n",
       "1   FOXA2  Q9Y261  0.517361  0.122593  6.342503  0.517361         1         1\n",
       "2   FOXL2  P58012  0.491874  0.098542  4.847039  0.491874         1         1\n",
       "3  HOXA13  P31271  0.483384  0.191965  9.279254  0.483384         1         1\n",
       "4   HOXA3  O43365  0.461362  0.149816  6.911944  0.461362         1         1\n",
       "5   FOXA1  P55317  0.457259  0.052730  2.411128  0.457259         1         1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#mapped sequence between protein and genes.\n",
    "predicted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce640a19-6030-4718-9a16-ac67693e5139",
   "metadata": {},
   "source": [
    "DATA PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534caffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gene</th>\n",
       "      <th>Protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FOXA2</td>\n",
       "      <td>Q9Y261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FOXL2</td>\n",
       "      <td>P58012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOXA13</td>\n",
       "      <td>P31271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOXA3</td>\n",
       "      <td>O43365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FOXA1</td>\n",
       "      <td>P55317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Gene Protein\n",
       "1   FOXA2  Q9Y261\n",
       "2   FOXL2  P58012\n",
       "3  HOXA13  P31271\n",
       "4   HOXA3  O43365\n",
       "5   FOXA1  P55317"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droping unwanted columns as we only need gene and proteins\n",
    "columns_to_drop = ['pred.seq', 'pred.ge', 'cons1', 'cons2', 'desall.g','desall.p']\n",
    "predicted_new = predicted.drop(columns = columns_to_drop)\n",
    "predicted_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b9db481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['A1BG', 'A1CF', 'A2BP1', 'A2LD1', 'A2M', 'A2ML1', 'A4GALT', 'A4GNT',\n",
      "       'AAA1', 'AAAS',\n",
      "       ...\n",
      "       'ZWILCH', 'ZWINT', 'ZXDA', 'ZXDB', 'ZXDC', 'ZYG11A', 'ZYG11B', 'ZYX',\n",
      "       'ZZEF', 'ZZZ3'],\n",
      "      dtype='object', length=20318)\n"
     ]
    }
   ],
   "source": [
    "# we need to check the index of Y and X \n",
    "index_Y = df_Y.index\n",
    "print(index_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c31225b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['q3dna', 'q3carb', 'q3atp', 'q3ppi', 'q3rna', 'top5dna', 'top5carb',\n",
      "       'top5atp', 'top5ppi', 'top5rna', 'top10dna', 'top10carb', 'top10atp',\n",
      "       'top10ppi', 'top10rna', 'top25dna', 'top25carb', 'top25atp', 'top25ppi',\n",
      "       'top25rna', 'countA', 'countC', 'countD', 'countE', 'countF', 'countG',\n",
      "       'countH', 'countI', 'countK', 'countL', 'countM', 'countN', 'countP',\n",
      "       'countQ', 'countR', 'countS', 'countT', 'countV', 'countW', 'countY',\n",
      "       'sqsize'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b29aac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GO:0003674', 'GO:0005576', 'GO:0005615', 'GO:0008150', 'GO:0070062',\n",
      "       'GO:0000166', 'GO:0003676', 'GO:0003723', 'GO:0005515', 'GO:0005654',\n",
      "       ...\n",
      "       'GO:0045892', 'GO:0003924', 'GO:0000122', 'GO:0004871', 'GO:0043565',\n",
      "       'GO:0006367', 'GO:0016567', 'GO:0004842', 'GO:0034220', 'GO:0005874'],\n",
      "      dtype='object', length=138)\n"
     ]
    }
   ],
   "source": [
    "print(df_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f020d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q3dna        0\n",
      "q3carb       0\n",
      "q3atp        0\n",
      "q3ppi        0\n",
      "q3rna        0\n",
      "top5dna      0\n",
      "top5carb     0\n",
      "top5atp      0\n",
      "top5ppi      0\n",
      "top5rna      0\n",
      "top10dna     0\n",
      "top10carb    0\n",
      "top10atp     0\n",
      "top10ppi     0\n",
      "top10rna     0\n",
      "top25dna     0\n",
      "top25carb    0\n",
      "top25atp     0\n",
      "top25ppi     0\n",
      "top25rna     0\n",
      "countA       0\n",
      "countC       0\n",
      "countD       0\n",
      "countE       0\n",
      "countF       0\n",
      "countG       0\n",
      "countH       0\n",
      "countI       0\n",
      "countK       0\n",
      "countL       0\n",
      "countM       0\n",
      "countN       0\n",
      "countP       0\n",
      "countQ       0\n",
      "countR       0\n",
      "countS       0\n",
      "countT       0\n",
      "countV       0\n",
      "countW       0\n",
      "countY       0\n",
      "sqsize       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for any missing values \n",
    "missing_values = df_X.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bb7dc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO:0003674    1\n",
      "GO:0005576    1\n",
      "GO:0005615    1\n",
      "GO:0008150    1\n",
      "GO:0070062    1\n",
      "             ..\n",
      "GO:0006367    1\n",
      "GO:0016567    1\n",
      "GO:0004842    1\n",
      "GO:0034220    1\n",
      "GO:0005874    1\n",
      "Length: 138, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#checking for any missing values \n",
    "missing_values = df_Y.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b409d2aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene       0\n",
      "Protein    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = predicted_new.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f14b064-cb58-46d1-a8d1-c9567e22174c",
   "metadata": {},
   "source": [
    "FEATURE EXTRACTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2911253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1BG      5.0\n",
      "A1CF      8.0\n",
      "A2M      10.0\n",
      "A4GNT     4.0\n",
      "AAAS     17.0\n",
      "         ... \n",
      "ZXDA      7.0\n",
      "ZXDB      7.0\n",
      "ZXDC      8.0\n",
      "ZYX      12.0\n",
      "ZZZ3     10.0\n",
      "Name: Sum, Length: 12504, dtype: float64\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 12504 entries, A1BG to ZZZ3\n",
      "Columns: 139 entries, GO:0003674 to Sum\n",
      "dtypes: float64(139)\n",
      "memory usage: 13.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Calculating the sum of 1 in Y and creating a new column, this is done to reduce the unwanted data and retain only the useful data\n",
    "df_Y['Sum'] = df_Y.iloc[:, :-1].sum(axis=1)\n",
    "filtered_df_Y = df_Y[df_Y['Sum'] >= 3]\n",
    "print(filtered_df_Y.iloc[:, -1])\n",
    "filtered_df_Y.info()\n",
    "#filtered_df_Y = filtered_df_Y.drop(columns='Sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a877be5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1         FOXA2\n",
      "2         FOXL2\n",
      "3        HOXA13\n",
      "4         HOXA3\n",
      "5         FOXA1\n",
      "          ...  \n",
      "15679      PRNP\n",
      "15680     CCL22\n",
      "15681    NDUFB1\n",
      "15682    PSENEN\n",
      "15683     TCL1A\n",
      "Name: Gene, Length: 12517, dtype: object\n"
     ]
    }
   ],
   "source": [
    "genes = predicted_new['Gene']\n",
    "#checking for genes\n",
    "index_Y1 = filtered_df_Y.index \n",
    "gene1 = genes\n",
    "geneY = index_Y1 \n",
    "common_indices_1 = list(gene1[gene1.isin(geneY)].index)\n",
    "common_genes = genes.loc[common_indices_1]\n",
    "unmatched_genes = genes[~genes.index.isin(common_indices_1)]\n",
    "print(common_genes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93daa60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Gene Protein\n",
      "1       FOXA2  Q9Y261\n",
      "2       FOXL2  P58012\n",
      "3      HOXA13  P31271\n",
      "4       HOXA3  O43365\n",
      "5       FOXA1  P55317\n",
      "...       ...     ...\n",
      "15679    PRNP  F7VJQ1\n",
      "15680   CCL22  O00626\n",
      "15681  NDUFB1  O75438\n",
      "15682  PSENEN  Q9NZ42\n",
      "15683   TCL1A  P56279\n",
      "\n",
      "[12517 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#matching the common genes to the predicted data to retrive the mappign proteins \n",
    "matching_proteins = predicted_new[predicted_new['Gene'].isin(common_genes)]\n",
    "matching_proteins_without_genes = matching_proteins.drop(columns=['Gene'])\n",
    "print(matching_proteins)\n",
    "#see the dimension of common_genes and matching_proteins are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22aeb5bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['P31946', 'P62258', 'Q04917', 'P61981', 'P31947', 'P27348', 'P63104',\n",
      "       'P30443', 'P01892', 'P04439',\n",
      "       ...\n",
      "       'Q9H900', 'O95229', 'P98168', 'P98169', 'Q2QGD7', 'Q6WRX3', 'Q9C0D3',\n",
      "       'Q15942', 'O43149', 'Q8IYH5'],\n",
      "      dtype='object', length=20177)\n"
     ]
    }
   ],
   "source": [
    "#the index of X has proteins, printing to check if the right command is used.\n",
    "index_X1 = df_X.index \n",
    "print(index_X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e6cc9b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Protein\n",
      "1      Q9Y261\n",
      "2      P58012\n",
      "3      P31271\n",
      "4      O43365\n",
      "5      P55317\n",
      "...       ...\n",
      "15679  F7VJQ1\n",
      "15680  O00626\n",
      "15681  O75438\n",
      "15682  Q9NZ42\n",
      "15683  P56279\n",
      "\n",
      "[12517 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#extracting common proteins\n",
    "protein2 = matching_proteins_without_genes\n",
    "common_proteins = protein2[protein2['Protein'].isin(index_X1)]\n",
    "uncommon_proteins = protein2[~protein2['Protein'].isin(index_X1)]\n",
    "\n",
    "print(common_proteins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33747d31",
   "metadata": {},
   "source": [
    "All genes and proteins are matched with the dataset. Both matching shows 0 unmatched proteins and genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3565d6b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           q3dna    q3carb     q3atp     q3ppi     q3rna  top5dna  top5carb  \\\n",
      "P31946  0.310625  0.185669  0.103394  0.346282  0.015200   0.7790  0.625905   \n",
      "P62258  0.311250  0.168360  0.114797  0.349137  0.016576   0.8475  0.649310   \n",
      "Q04917  0.459375  0.155227  0.129250  0.335464  0.018188   0.8805  0.660605   \n",
      "P61981  0.320000  0.189556  0.113903  0.345894  0.016431   0.8315  0.564121   \n",
      "P31947  0.501250  0.113197  0.144869  0.353664  0.017850   0.9150  0.618908   \n",
      "...          ...       ...       ...       ...       ...      ...       ...   \n",
      "P98168  0.453750  0.050854  0.130541  0.396634  0.009704   0.8825  0.448144   \n",
      "P98169  0.455000  0.051305  0.130560  0.399533  0.009704   0.8820  0.458804   \n",
      "Q2QGD7  0.495000  0.046613  0.128091  0.409245  0.009704   0.9155  0.452073   \n",
      "Q15942  0.425000  0.060512  0.122325  0.400090  0.009885   0.9035  0.590263   \n",
      "Q8IYH5  0.420000  0.050573  0.117857  0.407653  0.009692   0.9065  0.365448   \n",
      "\n",
      "         top5atp   top5ppi   top5rna  ...    countN    countP    countQ  \\\n",
      "P31946  0.176510  0.503740  0.070297  ...  0.056911  0.020325  0.060976   \n",
      "P62258  0.156503  0.493863  0.074668  ...  0.043137  0.019608  0.035294   \n",
      "Q04917  0.160915  0.457920  0.091139  ...  0.056911  0.016260  0.052846   \n",
      "P61981  0.175618  0.482345  0.076959  ...  0.056680  0.016194  0.048583   \n",
      "P31947  0.169927  0.454528  0.068090  ...  0.032258  0.028226  0.036290   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "P98168  0.238993  0.575000  0.022177  ...  0.027534  0.087610  0.046308   \n",
      "P98169  0.235669  0.587312  0.022570  ...  0.026152  0.090909  0.043587   \n",
      "Q2QGD7  0.256802  0.548478  0.027585  ...  0.020979  0.100233  0.046620   \n",
      "Q15942  0.240769  0.515402  0.035685  ...  0.020979  0.181818  0.080420   \n",
      "Q8IYH5  0.358429  0.570998  0.021989  ...  0.052049  0.067553  0.057586   \n",
      "\n",
      "          countR    countS    countT    countV    countW    countY    sqsize  \n",
      "P31946  0.040650  0.077236  0.048780  0.044715  0.008130  0.044715  2.390935  \n",
      "P62258  0.054902  0.054902  0.039216  0.050980  0.007843  0.047059  2.406540  \n",
      "Q04917  0.048780  0.069106  0.032520  0.052846  0.008130  0.048780  2.390935  \n",
      "P61981  0.052632  0.072874  0.044534  0.060729  0.008097  0.056680  2.392697  \n",
      "P31947  0.052419  0.076613  0.040323  0.048387  0.008065  0.040323  2.394452  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "P98168  0.046308  0.098874  0.058824  0.043805  0.006258  0.007509  2.902547  \n",
      "P98169  0.043587  0.095890  0.062267  0.044832  0.006227  0.007472  2.904716  \n",
      "Q2QGD7  0.055944  0.108392  0.050117  0.050117  0.003497  0.010490  2.933487  \n",
      "Q15942  0.040210  0.071678  0.041958  0.057692  0.001748  0.017483  2.757396  \n",
      "Q8IYH5  0.065338  0.106312  0.052049  0.060908  0.005537  0.025471  2.955688  \n",
      "\n",
      "[12504 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "# Extract the list of proteins from 'common_proteins'\n",
    "common_proteins_list = common_proteins['Protein'].tolist()\n",
    "\n",
    "# Filter the 'df_X' dataset for rows where the 'Protein' column matches the proteins in 'common_proteins'\n",
    "filtered_df_X = df_X[df_X.index.isin(common_proteins_list)]  # Assuming 'df_X' has a 'Protein' index\n",
    "\n",
    "print(filtered_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d461a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GO:0003674', 'GO:0005576', 'GO:0005615', 'GO:0008150', 'GO:0070062',\n",
      "       'GO:0000166', 'GO:0003676', 'GO:0003723', 'GO:0005515', 'GO:0005654',\n",
      "       ...\n",
      "       'GO:0003924', 'GO:0000122', 'GO:0004871', 'GO:0043565', 'GO:0006367',\n",
      "       'GO:0016567', 'GO:0004842', 'GO:0034220', 'GO:0005874', 'Sum'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "#these are the GO terms \n",
    "print(filtered_df_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed19ca28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value 'P31946' found at location: (7142, 'Protein')\n",
      "The gene associated with the protein P31946 is: YWHAB\n"
     ]
    }
   ],
   "source": [
    "#trying out for one value\n",
    "#looking for the 1st protein value \n",
    "value_to_find = filtered_df_X.index[0]\n",
    "column_name_1 = 'Protein'\n",
    "# make a boolean mask for the location of the value\n",
    "mask = matching_proteins[column_name_1] == value_to_find\n",
    "\n",
    "# Check if the value exists in predicted\n",
    "if mask.any():\n",
    "    # Get the location of the value\n",
    "    location = matching_proteins[mask].index[0], column_name_1\n",
    "    print(f\"Value '{value_to_find}' found at location: {location}\")\n",
    "else:\n",
    "    print(f\"Value '{value_to_find}' not found in 'predicted' dataset.\")\n",
    "    \n",
    "#Get the row \n",
    "row_number, column_name_1 = location\n",
    "\n",
    "gene_associated = matching_proteins.loc[row_number, 'Gene']\n",
    "\n",
    "print(f\"The gene associated with the protein {value_to_find} is: {gene_associated}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8fdb23fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = []\n",
    "genes = []  \n",
    "\n",
    "for value_to_find in filtered_df_X.index:\n",
    "    # Create a boolean mask for the location of the value in 'matching_proteins'\n",
    "    mask = matching_proteins['Protein'] == value_to_find\n",
    "    \n",
    "    # Check if the value exists in 'matching_proteins'\n",
    "    if mask.any():\n",
    "        # Get the location of the value\n",
    "        location = matching_proteins[mask].index[0]\n",
    "        \n",
    "        # Get the gene associated with the protein\n",
    "        gene_associated = matching_proteins.loc[location, 'Gene']\n",
    "        \n",
    "        # Store the value and associated gene in separate lists\n",
    "        values.append(value_to_find)\n",
    "        genes.append(gene_associated)\n",
    "    else:\n",
    "        # If the value isn't found, store None or any default value in both lists\n",
    "        values.append(value_to_find)\n",
    "        genes.append(None)  # Or set it to a default value\n",
    "\n",
    "values_and_genes = list(zip(values, genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e29c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No 'None' values found in tuples.\n"
     ]
    }
   ],
   "source": [
    "# Check if any tuple contains a 'None' value\n",
    "#making sure no none value \n",
    "none_present = any(None in tup for tup in values_and_genes)\n",
    "\n",
    "if none_present:\n",
    "    print(\"At least one tuple contains a 'None' value.\")\n",
    "else:\n",
    "    print(\"No 'None' values found in tuples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "236e1446",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1BG     1.0\n",
      "A1CF     0.0\n",
      "A2M      0.0\n",
      "A4GNT    0.0\n",
      "AAAS     1.0\n",
      "        ... \n",
      "ZXDA     0.0\n",
      "ZXDB     1.0\n",
      "ZXDC     0.0\n",
      "ZYX      0.0\n",
      "ZZZ3     0.0\n",
      "Name: GO:0003674, Length: 12504, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "column_1 = filtered_df_Y.iloc[:, 0]\n",
    "print(column_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c007d7e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['P31946', 'P62258', 'Q04917', 'P61981', 'P31947', 'P27348', 'P63104',\n",
      "       'P30443', 'P04439', 'P01889',\n",
      "       ...\n",
      "       'Q8NEG5', 'Q19AV6', 'O43264', 'Q9H900', 'O95229', 'P98168', 'P98169',\n",
      "       'Q2QGD7', 'Q15942', 'Q8IYH5'],\n",
      "      dtype='object', length=12504)\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df_X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dc8a2e7-7db4-4f11-862b-c54b86bc27bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           q3dna    q3carb     q3atp     q3ppi     q3rna  top5dna  top5carb  \\\n",
      "Q9Y261  0.425000  0.111431  0.128269  0.378110  0.014373   0.8665  0.558836   \n",
      "P58012  0.511250  0.103593  0.132917  0.376940  0.021208   0.8805  0.655053   \n",
      "P31271  0.480000  0.110199  0.129364  0.383290  0.017849   0.8700  0.593949   \n",
      "O43365  0.502500  0.076207  0.129867  0.384746  0.014981   0.8765  0.495063   \n",
      "P55317  0.452500  0.106916  0.129924  0.381331  0.015033   0.8770  0.541659   \n",
      "...          ...       ...       ...       ...       ...      ...       ...   \n",
      "F7VJQ1  0.640000  0.525451  0.150363  0.404775  0.049084   0.8415  0.889300   \n",
      "O00626  0.397500  0.109026  0.135152  0.411596  0.026502   0.8460  0.553895   \n",
      "O75438  0.323125  0.155844  0.138801  0.400135  0.025632   0.7750  0.503216   \n",
      "Q9NZ42  0.417500  0.179703  0.149878  0.413406  0.025306   0.8465  0.646778   \n",
      "P56279  0.363125  0.094064  0.134547  0.385972  0.026903   0.8190  0.559921   \n",
      "\n",
      "         top5atp   top5ppi   top5rna  ...    countN    countP    countQ  \\\n",
      "Q9Y261  0.235763  0.537569  0.062264  ...  0.041575  0.091904  0.052516   \n",
      "P58012  0.289709  0.532134  0.095771  ...  0.026596  0.162234  0.029255   \n",
      "P31271  0.266975  0.576419  0.055814  ...  0.033505  0.095361  0.033505   \n",
      "O43365  0.296282  0.539430  0.057661  ...  0.038375  0.169300  0.060948   \n",
      "P55317  0.213419  0.531217  0.058776  ...  0.048729  0.091102  0.042373   \n",
      "...          ...       ...       ...  ...       ...       ...       ...   \n",
      "F7VJQ1  0.194899  0.519289  0.104356  ...  0.000000  0.095890  0.068493   \n",
      "O00626  0.215562  0.481926  0.089155  ...  0.021505  0.064516  0.032258   \n",
      "O75438  0.155775  0.490534  0.079276  ...  0.034483  0.034483  0.034483   \n",
      "Q9NZ42  0.187371  0.497228  0.076084  ...  0.039604  0.049505  0.029703   \n",
      "P56279  0.167992  0.471362  0.075383  ...  0.000000  0.078947  0.035088   \n",
      "\n",
      "          countR    countS    countT    countV    countW    countY    sqsize  \n",
      "Q9Y261  0.028446  0.120350  0.026258  0.017505  0.008753  0.045952  2.659916  \n",
      "P58012  0.042553  0.055851  0.023936  0.023936  0.010638  0.042553  2.575188  \n",
      "P31271  0.041237  0.074742  0.030928  0.033505  0.012887  0.041237  2.588832  \n",
      "O43365  0.033860  0.110609  0.045147  0.020316  0.004515  0.047404  2.646404  \n",
      "P55317  0.029661  0.133475  0.055085  0.025424  0.008475  0.046610  2.673942  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "F7VJQ1  0.054795  0.082192  0.027397  0.013699  0.232877  0.000000  1.863323  \n",
      "O00626  0.086022  0.043011  0.043011  0.129032  0.021505  0.043011  1.968483  \n",
      "O75438  0.086207  0.051724  0.034483  0.120690  0.034483  0.017241  1.763428  \n",
      "Q9NZ42  0.059406  0.049505  0.049505  0.069307  0.059406  0.059406  2.004321  \n",
      "P56279  0.078947  0.043860  0.043860  0.061404  0.043860  0.035088  2.056905  \n",
      "\n",
      "[12496 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "common_proteins2 = filtered_df_X.index.intersection(matching_proteins['Protein'])\n",
    "\n",
    "filtered_df_X = filtered_df_X.loc[common_proteins_list].drop_duplicates()\n",
    "matching_proteins_dropped_duplicates = matching_proteins[matching_proteins['Protein'].isin(common_proteins2)].drop_duplicates(subset='Protein')\n",
    "\n",
    "# Arrange matching_proteins based on the index of filtered_df_X\n",
    "matching_proteins2 = matching_proteins_dropped_duplicates.set_index('Protein').loc[filtered_df_X.index].reset_index()\n",
    "\n",
    "print(filtered_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72c8552b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GO:0003674', 'GO:0005576', 'GO:0005615', 'GO:0008150', 'GO:0070062',\n",
      "       'GO:0000166', 'GO:0003676', 'GO:0003723', 'GO:0005515', 'GO:0005654',\n",
      "       ...\n",
      "       'GO:0003924', 'GO:0000122', 'GO:0004871', 'GO:0043565', 'GO:0006367',\n",
      "       'GO:0016567', 'GO:0004842', 'GO:0034220', 'GO:0005874', 'Sum'],\n",
      "      dtype='object', length=139)\n"
     ]
    }
   ],
   "source": [
    "print(df_Y.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bdd5ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_dict = {}\n",
    "\n",
    "for i, column in enumerate(df_Y.columns, 1):\n",
    "    column_name = f\"column_{i}\"\n",
    "    columns_dict[column_name] = df_Y[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f14abb4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arranged column based on 'Gene' column of mapped dataset:\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#column1 = filtered_df_Y['GO:0003674'] \n",
    "#trying for 1 value\n",
    "arranged_column = columns_dict[\"column_1\"].reindex(matching_proteins2['Gene']).values\n",
    "\n",
    "print(\"Arranged column based on 'Gene' column of mapped dataset:\")\n",
    "print(arranged_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92fa288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arranged Column: column_1\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_2\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_3\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_4\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_5\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Arranged Column: column_6\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_7\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_8\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_9\n",
      "[0. 1. 0. ... 0. 1. 1.]\n",
      "Arranged Column: column_10\n",
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_11\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_12\n",
      "[0. 0. 0. ... 0. 1. 1.]\n",
      "Arranged Column: column_13\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_14\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_15\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_16\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_17\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_18\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_19\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_20\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_21\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_22\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_23\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_24\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Arranged Column: column_25\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_26\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_27\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_28\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_29\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_30\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_31\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_32\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_33\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_34\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Arranged Column: column_35\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_36\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_37\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_38\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_39\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_40\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_41\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_42\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_43\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_44\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_45\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_46\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_47\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_48\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_49\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_50\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_51\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_52\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_53\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_54\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_55\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_56\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_57\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_58\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_59\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_60\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_61\n",
      "[0. 1. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_62\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_63\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_64\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_65\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_66\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_67\n",
      "[0. 0. 0. ... 1. 0. 0.]\n",
      "Arranged Column: column_68\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_69\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_70\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_71\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_72\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_73\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_74\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_75\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_76\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_77\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_78\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_79\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_80\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_81\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_82\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_83\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_84\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_85\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_86\n",
      "[1. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_87\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_88\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_89\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_90\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_91\n",
      "[0. 0. 0. ... 0. 1. 0.]\n",
      "Arranged Column: column_92\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_93\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_94\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_95\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_96\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_97\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_98\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_99\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_100\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_101\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_102\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_103\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_104\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_105\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_106\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_107\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_108\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_109\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_110\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_111\n",
      "[0. 0. 0. ... 0. 0. 1.]\n",
      "Arranged Column: column_112\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_113\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_114\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_115\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_116\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_117\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_118\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_119\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_120\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_121\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_122\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_123\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_124\n",
      "[1. 1. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_125\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_126\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_127\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_128\n",
      "[1. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_129\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_130\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_131\n",
      "[0. 1. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_132\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_133\n",
      "[0. 1. 1. ... 0. 0. 0.]\n",
      "Arranged Column: column_134\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_135\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_136\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_137\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Arranged Column: column_138\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "arranged_columns = {}  # Dictionary to store the reindexed columns\n",
    "\n",
    "for i in range(1, 139): #last column is sum column \n",
    "    column_name = f\"column_{i}\"\n",
    "    if column_name in columns_dict:\n",
    "        arranged_columns[column_name] = columns_dict[column_name].reindex(matching_proteins2['Gene']).values\n",
    "    else:\n",
    "        print(f\"Column {column_name} not found in columns_dict.\")\n",
    "\n",
    "# Printing or using the arranged columns\n",
    "for column_name, arranged_column in arranged_columns.items():\n",
    "    print(f\"Arranged Column: {column_name}\")\n",
    "    print(arranged_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da329091",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.Series(arranged_columns['column_1'], index=filtered_df_X.index, name = filtered_df_Y.columns[0])\n",
    "\n",
    "result_data_1 = pd.concat([filtered_df_X, data_1],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74b858c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data_dict = {}\n",
    "\n",
    "for i in range(1,139):\n",
    "    column_name = f'column_{i}'\n",
    "    if column_name in arranged_columns:\n",
    "        data_i = pd.Series(arranged_columns[column_name], index=filtered_df_X.index, name = filtered_df_Y.columns[i - 1])\n",
    "        result_data_i = pd.concat([filtered_df_X, data_i],axis = 1)\n",
    "        result_data_dict[f\"result_data_{i}\"] = result_data_i\n",
    "#print(result_data_dict.keys())\n",
    "#print(result_data_dict['result_data_3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09892053-6921-4de6-8b6a-ae098dfa256f",
   "metadata": {},
   "source": [
    "MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52259e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data: 0.9564825930372148\n",
      "Accuracy of the logistic regression model for test data: 0.9604\n"
     ]
    }
   ],
   "source": [
    "#for a single dataset \n",
    "X1 = result_data_dict['result_data_1'].drop('GO:0003674', axis=1) \n",
    "y1 = result_data_dict['result_data_1']['GO:0003674']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( X1,y1, test_size=0.2, random_state=42) \n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "predictions = logistic_regression.predict(X_test)\n",
    "print(predictions)\n",
    "\n",
    "accuracy_train = logistic_regression.score(X_train,y_train)\n",
    "print(f\"Accuracy of the logistic regression model for train data: {accuracy_train}\")\n",
    "\n",
    "accuracy_test = logistic_regression.score(X_test, y_test)\n",
    "print(f\"Accuracy of the logistic regression model for test data: {accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82a00ef1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for result_data_1: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_1: 0.9564825930372148\n",
      "Accuracy of the logistic regression model for test data for result_data_1: 0.9604\n",
      "Predictions for result_data_2: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_2: 0.9199679871948779\n",
      "Accuracy of the logistic regression model for test data for result_data_2: 0.93\n",
      "Predictions for result_data_3: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_3: 0.921468587434974\n",
      "Accuracy of the logistic regression model for test data for result_data_3: 0.9216\n",
      "Predictions for result_data_4: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_4: 0.9592837134853942\n",
      "Accuracy of the logistic regression model for test data for result_data_4: 0.9692\n",
      "Predictions for result_data_5: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_5: 0.8123249299719888\n",
      "Accuracy of the logistic regression model for test data for result_data_5: 0.816\n",
      "Predictions for result_data_6: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_6: 0.9810924369747899\n",
      "Accuracy of the logistic regression model for test data for result_data_6: 0.9848\n",
      "Predictions for result_data_7: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_7: 0.9843937575030012\n",
      "Accuracy of the logistic regression model for test data for result_data_7: 0.9844\n",
      "Predictions for result_data_8: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_8: 0.974389755902361\n",
      "Accuracy of the logistic regression model for test data for result_data_8: 0.9752\n",
      "Predictions for result_data_9: [1. 1. 1. ... 0. 1. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_9: 0.626750700280112\n",
      "Accuracy of the logistic regression model for test data for result_data_9: 0.6128\n",
      "Predictions for result_data_10: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_10: 0.8217286914765907\n",
      "Accuracy of the logistic regression model for test data for result_data_10: 0.8088\n",
      "Predictions for result_data_11: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_11: 0.6810724289715886\n",
      "Accuracy of the logistic regression model for test data for result_data_11: 0.6856\n",
      "Predictions for result_data_12: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_12: 0.947579031612645\n",
      "Accuracy of the logistic regression model for test data for result_data_12: 0.9508\n",
      "Predictions for result_data_13: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_13: 0.9387755102040817\n",
      "Accuracy of the logistic regression model for test data for result_data_13: 0.9384\n",
      "Predictions for result_data_14: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_14: 0.9770908363345339\n",
      "Accuracy of the logistic regression model for test data for result_data_14: 0.9736\n",
      "Predictions for result_data_15: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_15: 0.7800120048019208\n",
      "Accuracy of the logistic regression model for test data for result_data_15: 0.7784\n",
      "Predictions for result_data_16: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_16: 0.9662865146058424\n",
      "Accuracy of the logistic regression model for test data for result_data_16: 0.9724\n",
      "Predictions for result_data_17: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_17: 0.9651860744297719\n",
      "Accuracy of the logistic regression model for test data for result_data_17: 0.958\n",
      "Predictions for result_data_18: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_18: 0.9782913165266106\n",
      "Accuracy of the logistic regression model for test data for result_data_18: 0.9752\n",
      "Predictions for result_data_19: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_19: 0.984093637454982\n",
      "Accuracy of the logistic regression model for test data for result_data_19: 0.982\n",
      "Predictions for result_data_20: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_20: 0.976890756302521\n",
      "Accuracy of the logistic regression model for test data for result_data_20: 0.9776\n",
      "Predictions for result_data_21: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_21: 0.8609443777511004\n",
      "Accuracy of the logistic regression model for test data for result_data_21: 0.8652\n",
      "Predictions for result_data_22: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_22: 0.9660864345738295\n",
      "Accuracy of the logistic regression model for test data for result_data_22: 0.9608\n",
      "Predictions for result_data_23: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_23: 0.9742897158863545\n",
      "Accuracy of the logistic regression model for test data for result_data_23: 0.9792\n",
      "Predictions for result_data_24: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_24: 0.8346338535414166\n",
      "Accuracy of the logistic regression model for test data for result_data_24: 0.834\n",
      "Predictions for result_data_25: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_25: 0.9690876350540216\n",
      "Accuracy of the logistic regression model for test data for result_data_25: 0.9764\n",
      "Predictions for result_data_26: [1. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_26: 0.7140856342537015\n",
      "Accuracy of the logistic regression model for test data for result_data_26: 0.7052\n",
      "Predictions for result_data_27: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_27: 0.974389755902361\n",
      "Accuracy of the logistic regression model for test data for result_data_27: 0.9764\n",
      "Predictions for result_data_28: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_28: 0.9761904761904762\n",
      "Accuracy of the logistic regression model for test data for result_data_28: 0.9768\n",
      "Predictions for result_data_29: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_29: 0.9570828331332533\n",
      "Accuracy of the logistic regression model for test data for result_data_29: 0.962\n",
      "Predictions for result_data_30: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_30: 0.9802921168467387\n",
      "Accuracy of the logistic regression model for test data for result_data_30: 0.9788\n",
      "Predictions for result_data_31: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_31: 0.9843937575030012\n",
      "Accuracy of the logistic regression model for test data for result_data_31: 0.988\n",
      "Predictions for result_data_32: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_32: 0.9764905962384954\n",
      "Accuracy of the logistic regression model for test data for result_data_32: 0.9772\n",
      "Predictions for result_data_33: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_33: 0.951280512204882\n",
      "Accuracy of the logistic regression model for test data for result_data_33: 0.9584\n",
      "Predictions for result_data_34: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_34: 0.8925570228091236\n",
      "Accuracy of the logistic regression model for test data for result_data_34: 0.8936\n",
      "Predictions for result_data_35: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_35: 0.9543817527010804\n",
      "Accuracy of the logistic regression model for test data for result_data_35: 0.9492\n",
      "Predictions for result_data_36: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_36: 0.898359343737495\n",
      "Accuracy of the logistic regression model for test data for result_data_36: 0.8988\n",
      "Predictions for result_data_37: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_37: 0.9759903961584634\n",
      "Accuracy of the logistic regression model for test data for result_data_37: 0.9796\n",
      "Predictions for result_data_38: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_38: 0.9473789515806322\n",
      "Accuracy of the logistic regression model for test data for result_data_38: 0.9432\n",
      "Predictions for result_data_39: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_39: 0.9566826730692277\n",
      "Accuracy of the logistic regression model for test data for result_data_39: 0.9568\n",
      "Predictions for result_data_40: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_40: 0.9808923569427771\n",
      "Accuracy of the logistic regression model for test data for result_data_40: 0.9796\n",
      "Predictions for result_data_41: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_41: 0.9503801520608244\n",
      "Accuracy of the logistic regression model for test data for result_data_41: 0.9492\n",
      "Predictions for result_data_42: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_42: 0.9765906362545018\n",
      "Accuracy of the logistic regression model for test data for result_data_42: 0.9724\n",
      "Predictions for result_data_43: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_43: 0.9646858743497398\n",
      "Accuracy of the logistic regression model for test data for result_data_43: 0.964\n",
      "Predictions for result_data_44: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_44: 0.9656862745098039\n",
      "Accuracy of the logistic regression model for test data for result_data_44: 0.9648\n",
      "Predictions for result_data_45: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_45: 0.9333733493397359\n",
      "Accuracy of the logistic regression model for test data for result_data_45: 0.9304\n",
      "Predictions for result_data_46: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_46: 0.9838935574229691\n",
      "Accuracy of the logistic regression model for test data for result_data_46: 0.982\n",
      "Predictions for result_data_47: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_47: 0.7718087234893958\n",
      "Accuracy of the logistic regression model for test data for result_data_47: 0.7684\n",
      "Predictions for result_data_48: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_48: 0.9658863545418167\n",
      "Accuracy of the logistic regression model for test data for result_data_48: 0.9556\n",
      "Predictions for result_data_49: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_49: 0.970688275310124\n",
      "Accuracy of the logistic regression model for test data for result_data_49: 0.966\n",
      "Predictions for result_data_50: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_50: 0.9572829131652661\n",
      "Accuracy of the logistic regression model for test data for result_data_50: 0.958\n",
      "Predictions for result_data_51: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_51: 0.9222689075630253\n",
      "Accuracy of the logistic regression model for test data for result_data_51: 0.9232\n",
      "Predictions for result_data_52: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_52: 0.9223689475790317\n",
      "Accuracy of the logistic regression model for test data for result_data_52: 0.9132\n",
      "Predictions for result_data_53: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_53: 0.8815526210484194\n",
      "Accuracy of the logistic regression model for test data for result_data_53: 0.8608\n",
      "Predictions for result_data_54: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_54: 0.9652861144457783\n",
      "Accuracy of the logistic regression model for test data for result_data_54: 0.9652\n",
      "Predictions for result_data_55: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_55: 0.9623849539815926\n",
      "Accuracy of the logistic regression model for test data for result_data_55: 0.9632\n",
      "Predictions for result_data_56: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_56: 0.9418767507002801\n",
      "Accuracy of the logistic regression model for test data for result_data_56: 0.9308\n",
      "Predictions for result_data_57: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_57: 0.9422769107643058\n",
      "Accuracy of the logistic regression model for test data for result_data_57: 0.9452\n",
      "Predictions for result_data_58: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_58: 0.9419767907162865\n",
      "Accuracy of the logistic regression model for test data for result_data_58: 0.9516\n",
      "Predictions for result_data_59: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_59: 0.9713885554221688\n",
      "Accuracy of the logistic regression model for test data for result_data_59: 0.9696\n",
      "Predictions for result_data_60: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_60: 0.9692877150860344\n",
      "Accuracy of the logistic regression model for test data for result_data_60: 0.9708\n",
      "Predictions for result_data_61: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_61: 0.978891556622649\n",
      "Accuracy of the logistic regression model for test data for result_data_61: 0.9784\n",
      "Predictions for result_data_62: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_62: 0.927170868347339\n",
      "Accuracy of the logistic regression model for test data for result_data_62: 0.922\n",
      "Predictions for result_data_63: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_63: 0.9793917567026811\n",
      "Accuracy of the logistic regression model for test data for result_data_63: 0.9788\n",
      "Predictions for result_data_64: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_64: 0.9688875550220089\n",
      "Accuracy of the logistic regression model for test data for result_data_64: 0.9632\n",
      "Predictions for result_data_65: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_65: 0.9160664265706282\n",
      "Accuracy of the logistic regression model for test data for result_data_65: 0.9136\n",
      "Predictions for result_data_66: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_66: 0.9645858343337335\n",
      "Accuracy of the logistic regression model for test data for result_data_66: 0.966\n",
      "Predictions for result_data_67: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_67: 0.9707883153261304\n",
      "Accuracy of the logistic regression model for test data for result_data_67: 0.9744\n",
      "Predictions for result_data_68: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_68: 0.9854941976790716\n",
      "Accuracy of the logistic regression model for test data for result_data_68: 0.9836\n",
      "Predictions for result_data_69: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_69: 0.9829931972789115\n",
      "Accuracy of the logistic regression model for test data for result_data_69: 0.9832\n",
      "Predictions for result_data_70: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_70: 0.9836934773909564\n",
      "Accuracy of the logistic regression model for test data for result_data_70: 0.9852\n",
      "Predictions for result_data_71: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_71: 0.9737895158063226\n",
      "Accuracy of the logistic regression model for test data for result_data_71: 0.9732\n",
      "Predictions for result_data_72: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_72: 0.978391356542617\n",
      "Accuracy of the logistic regression model for test data for result_data_72: 0.9784\n",
      "Predictions for result_data_73: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_73: 0.9809923969587835\n",
      "Accuracy of the logistic regression model for test data for result_data_73: 0.9788\n",
      "Predictions for result_data_74: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_74: 0.928171268507403\n",
      "Accuracy of the logistic regression model for test data for result_data_74: 0.9144\n",
      "Predictions for result_data_75: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_75: 0.9590836334533813\n",
      "Accuracy of the logistic regression model for test data for result_data_75: 0.9576\n",
      "Predictions for result_data_76: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_76: 0.9834933973589436\n",
      "Accuracy of the logistic regression model for test data for result_data_76: 0.9856\n",
      "Predictions for result_data_77: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_77: 0.9757903161264506\n",
      "Accuracy of the logistic regression model for test data for result_data_77: 0.9756\n",
      "Predictions for result_data_78: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_78: 0.9139655862344938\n",
      "Accuracy of the logistic regression model for test data for result_data_78: 0.8948\n",
      "Predictions for result_data_79: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_79: 0.969187675070028\n",
      "Accuracy of the logistic regression model for test data for result_data_79: 0.9728\n",
      "Predictions for result_data_80: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_80: 0.9714885954381752\n",
      "Accuracy of the logistic regression model for test data for result_data_80: 0.974\n",
      "Predictions for result_data_81: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_81: 0.9584833933573429\n",
      "Accuracy of the logistic regression model for test data for result_data_81: 0.9636\n",
      "Predictions for result_data_82: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_82: 0.9760904361744698\n",
      "Accuracy of the logistic regression model for test data for result_data_82: 0.976\n",
      "Predictions for result_data_83: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_83: 0.9720888355342137\n",
      "Accuracy of the logistic regression model for test data for result_data_83: 0.97\n",
      "Predictions for result_data_84: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_84: 0.9717887154861945\n",
      "Accuracy of the logistic regression model for test data for result_data_84: 0.9684\n",
      "Predictions for result_data_85: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_85: 0.9461784713885554\n",
      "Accuracy of the logistic regression model for test data for result_data_85: 0.9496\n",
      "Predictions for result_data_86: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_86: 0.898359343737495\n",
      "Accuracy of the logistic regression model for test data for result_data_86: 0.8808\n",
      "Predictions for result_data_87: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_87: 0.9797919167667066\n",
      "Accuracy of the logistic regression model for test data for result_data_87: 0.9824\n",
      "Predictions for result_data_88: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_88: 0.9819927971188476\n",
      "Accuracy of the logistic regression model for test data for result_data_88: 0.9808\n",
      "Predictions for result_data_89: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_89: 0.9844937975190076\n",
      "Accuracy of the logistic regression model for test data for result_data_89: 0.9848\n",
      "Predictions for result_data_90: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_90: 0.9837935174069627\n",
      "Accuracy of the logistic regression model for test data for result_data_90: 0.9848\n",
      "Predictions for result_data_91: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_91: 0.9725890356142457\n",
      "Accuracy of the logistic regression model for test data for result_data_91: 0.966\n",
      "Predictions for result_data_92: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_92: 0.9841936774709884\n",
      "Accuracy of the logistic regression model for test data for result_data_92: 0.9852\n",
      "Predictions for result_data_93: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_93: 0.9849939975990396\n",
      "Accuracy of the logistic regression model for test data for result_data_93: 0.9844\n",
      "Predictions for result_data_94: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_94: 0.9791916766706683\n",
      "Accuracy of the logistic regression model for test data for result_data_94: 0.9812\n",
      "Predictions for result_data_95: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_95: 0.8781512605042017\n",
      "Accuracy of the logistic regression model for test data for result_data_95: 0.8664\n",
      "Predictions for result_data_96: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_96: 0.9826930772308924\n",
      "Accuracy of the logistic regression model for test data for result_data_96: 0.9844\n",
      "Predictions for result_data_97: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_97: 0.970188075230092\n",
      "Accuracy of the logistic regression model for test data for result_data_97: 0.9744\n",
      "Predictions for result_data_98: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_98: 0.9811924769907964\n",
      "Accuracy of the logistic regression model for test data for result_data_98: 0.9816\n",
      "Predictions for result_data_99: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_99: 0.9608843537414966\n",
      "Accuracy of the logistic regression model for test data for result_data_99: 0.9596\n",
      "Predictions for result_data_100: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_100: 0.9841936774709884\n",
      "Accuracy of the logistic regression model for test data for result_data_100: 0.9852\n",
      "Predictions for result_data_101: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_101: 0.9836934773909564\n",
      "Accuracy of the logistic regression model for test data for result_data_101: 0.9824\n",
      "Predictions for result_data_102: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_102: 0.9766906762705082\n",
      "Accuracy of the logistic regression model for test data for result_data_102: 0.9812\n",
      "Predictions for result_data_103: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_103: 0.9674869947979192\n",
      "Accuracy of the logistic regression model for test data for result_data_103: 0.972\n",
      "Predictions for result_data_104: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_104: 0.9799919967987195\n",
      "Accuracy of the logistic regression model for test data for result_data_104: 0.9792\n",
      "Predictions for result_data_105: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_105: 0.9729891956782714\n",
      "Accuracy of the logistic regression model for test data for result_data_105: 0.9736\n",
      "Predictions for result_data_106: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_106: 0.9559823929571829\n",
      "Accuracy of the logistic regression model for test data for result_data_106: 0.9612\n",
      "Predictions for result_data_107: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_107: 0.9844937975190076\n",
      "Accuracy of the logistic regression model for test data for result_data_107: 0.986\n",
      "Predictions for result_data_108: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_108: 0.9806922769107643\n",
      "Accuracy of the logistic regression model for test data for result_data_108: 0.9792\n",
      "Predictions for result_data_109: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_109: 0.9736894757903162\n",
      "Accuracy of the logistic regression model for test data for result_data_109: 0.9668\n",
      "Predictions for result_data_110: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_110: 0.9760904361744698\n",
      "Accuracy of the logistic regression model for test data for result_data_110: 0.9748\n",
      "Predictions for result_data_111: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_111: 0.9717887154861945\n",
      "Accuracy of the logistic regression model for test data for result_data_111: 0.9712\n",
      "Predictions for result_data_112: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_112: 0.9778911564625851\n",
      "Accuracy of the logistic regression model for test data for result_data_112: 0.9828\n",
      "Predictions for result_data_113: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_113: 0.9742897158863545\n",
      "Accuracy of the logistic regression model for test data for result_data_113: 0.9736\n",
      "Predictions for result_data_114: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_114: 0.9787915166066427\n",
      "Accuracy of the logistic regression model for test data for result_data_114: 0.976\n",
      "Predictions for result_data_115: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_115: 0.974389755902361\n",
      "Accuracy of the logistic regression model for test data for result_data_115: 0.9732\n",
      "Predictions for result_data_116: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_116: 0.9811924769907964\n",
      "Accuracy of the logistic regression model for test data for result_data_116: 0.9796\n",
      "Predictions for result_data_117: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_117: 0.9880952380952381\n",
      "Accuracy of the logistic regression model for test data for result_data_117: 0.9828\n",
      "Predictions for result_data_118: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_118: 0.9713885554221688\n",
      "Accuracy of the logistic regression model for test data for result_data_118: 0.976\n",
      "Predictions for result_data_119: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_119: 0.9793917567026811\n",
      "Accuracy of the logistic regression model for test data for result_data_119: 0.9752\n",
      "Predictions for result_data_120: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_120: 0.954781912765106\n",
      "Accuracy of the logistic regression model for test data for result_data_120: 0.9568\n",
      "Predictions for result_data_121: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_121: 0.9794917967186875\n",
      "Accuracy of the logistic regression model for test data for result_data_121: 0.9808\n",
      "Predictions for result_data_122: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_122: 0.977390956382553\n",
      "Accuracy of the logistic regression model for test data for result_data_122: 0.9828\n",
      "Predictions for result_data_123: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_123: 0.9830932372949179\n",
      "Accuracy of the logistic regression model for test data for result_data_123: 0.9848\n",
      "Predictions for result_data_124: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_124: 0.9621848739495799\n",
      "Accuracy of the logistic regression model for test data for result_data_124: 0.9624\n",
      "Predictions for result_data_125: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_125: 0.9727891156462585\n",
      "Accuracy of the logistic regression model for test data for result_data_125: 0.9696\n",
      "Predictions for result_data_126: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_126: 0.9808923569427771\n",
      "Accuracy of the logistic regression model for test data for result_data_126: 0.982\n",
      "Predictions for result_data_127: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_127: 0.9785914365746299\n",
      "Accuracy of the logistic regression model for test data for result_data_127: 0.9716\n",
      "Predictions for result_data_128: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_128: 0.9797919167667066\n",
      "Accuracy of the logistic regression model for test data for result_data_128: 0.98\n",
      "Predictions for result_data_129: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_129: 0.9665866346538615\n",
      "Accuracy of the logistic regression model for test data for result_data_129: 0.9664\n",
      "Predictions for result_data_130: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_130: 0.9822929171668667\n",
      "Accuracy of the logistic regression model for test data for result_data_130: 0.98\n",
      "Predictions for result_data_131: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_131: 0.9488795518207283\n",
      "Accuracy of the logistic regression model for test data for result_data_131: 0.954\n",
      "Predictions for result_data_132: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_132: 0.9830932372949179\n",
      "Accuracy of the logistic regression model for test data for result_data_132: 0.9792\n",
      "Predictions for result_data_133: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_133: 0.9705882352941176\n",
      "Accuracy of the logistic regression model for test data for result_data_133: 0.9708\n",
      "Predictions for result_data_134: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_134: 0.9811924769907964\n",
      "Accuracy of the logistic regression model for test data for result_data_134: 0.9816\n",
      "Predictions for result_data_135: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_135: 0.9781912765106042\n",
      "Accuracy of the logistic regression model for test data for result_data_135: 0.9748\n",
      "Predictions for result_data_136: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_136: 0.9797919167667066\n",
      "Accuracy of the logistic regression model for test data for result_data_136: 0.9828\n",
      "Predictions for result_data_137: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_137: 0.9804921968787516\n",
      "Accuracy of the logistic regression model for test data for result_data_137: 0.9788\n",
      "Predictions for result_data_138: [0. 0. 0. ... 0. 0. 0.]\n",
      "Accuracy of the logistic regression model for train data for result_data_138: 0.982092837134854\n",
      "Accuracy of the logistic regression model for test data for result_data_138: 0.9848\n"
     ]
    }
   ],
   "source": [
    "# Loop through columns\n",
    "for i in range(1, 139):\n",
    "    # Drop the last column from X and assign X1, and select the last column for y1\n",
    "    last_column_index = -1  # Index of the last column\n",
    "    X1 = result_data_dict[f'result_data_{i}'].iloc[:, :-1]  # Dropping the last column\n",
    "    y1 = result_data_dict[f'result_data_{i}'].iloc[:, last_column_index]  # Selecting the last column as y\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Initialize Logistic Regression model\n",
    "    logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "    # Fit the model\n",
    "    model = logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = logistic_regression.predict(X_test)\n",
    "    print(f\"Predictions for result_data_{i}: {predictions}\")\n",
    "\n",
    "    # Calculate accuracy for train data\n",
    "    accuracy_train = logistic_regression.score(X_train, y_train)\n",
    "    print(f\"Accuracy of the logistic regression model for train data for result_data_{i}: {accuracy_train}\")\n",
    "\n",
    "    # Calculate accuracy for test data\n",
    "    accuracy_test = logistic_regression.score(X_test, y_test)\n",
    "    print(f\"Accuracy of the logistic regression model for test data for result_data_{i}: {accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7860f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy for result_data_1 after 5:2 cross-validation: 0.957\n",
      "Average accuracy for result_data_2 after 5:2 cross-validation: 0.922\n",
      "Average accuracy for result_data_3 after 5:2 cross-validation: 0.922\n",
      "Average accuracy for result_data_4 after 5:2 cross-validation: 0.961\n",
      "Average accuracy for result_data_5 after 5:2 cross-validation: 0.813\n",
      "Average accuracy for result_data_6 after 5:2 cross-validation: 0.982\n",
      "Average accuracy for result_data_7 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_8 after 5:2 cross-validation: 0.975\n",
      "Average accuracy for result_data_9 after 5:2 cross-validation: 0.619\n",
      "Average accuracy for result_data_10 after 5:2 cross-validation: 0.819\n",
      "Average accuracy for result_data_11 after 5:2 cross-validation: 0.681\n",
      "Average accuracy for result_data_12 after 5:2 cross-validation: 0.948\n",
      "Average accuracy for result_data_13 after 5:2 cross-validation: 0.939\n",
      "Average accuracy for result_data_14 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_15 after 5:2 cross-validation: 0.78\n",
      "Average accuracy for result_data_16 after 5:2 cross-validation: 0.968\n",
      "Average accuracy for result_data_17 after 5:2 cross-validation: 0.964\n",
      "Average accuracy for result_data_18 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_19 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_20 after 5:2 cross-validation: 0.977\n",
      "Average accuracy for result_data_21 after 5:2 cross-validation: 0.862\n",
      "Average accuracy for result_data_22 after 5:2 cross-validation: 0.965\n",
      "Average accuracy for result_data_23 after 5:2 cross-validation: 0.975\n",
      "Average accuracy for result_data_24 after 5:2 cross-validation: 0.832\n",
      "Average accuracy for result_data_25 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_26 after 5:2 cross-validation: 0.709\n",
      "Average accuracy for result_data_27 after 5:2 cross-validation: 0.975\n",
      "Average accuracy for result_data_28 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_29 after 5:2 cross-validation: 0.958\n",
      "Average accuracy for result_data_30 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_31 after 5:2 cross-validation: 0.985\n",
      "Average accuracy for result_data_32 after 5:2 cross-validation: 0.977\n",
      "Average accuracy for result_data_33 after 5:2 cross-validation: 0.953\n",
      "Average accuracy for result_data_34 after 5:2 cross-validation: 0.893\n",
      "Average accuracy for result_data_35 after 5:2 cross-validation: 0.953\n",
      "Average accuracy for result_data_36 after 5:2 cross-validation: 0.898\n",
      "Average accuracy for result_data_37 after 5:2 cross-validation: 0.977\n",
      "Average accuracy for result_data_38 after 5:2 cross-validation: 0.947\n",
      "Average accuracy for result_data_39 after 5:2 cross-validation: 0.957\n",
      "Average accuracy for result_data_40 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_41 after 5:2 cross-validation: 0.95\n",
      "Average accuracy for result_data_42 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_43 after 5:2 cross-validation: 0.965\n",
      "Average accuracy for result_data_44 after 5:2 cross-validation: 0.966\n",
      "Average accuracy for result_data_45 after 5:2 cross-validation: 0.933\n",
      "Average accuracy for result_data_46 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_47 after 5:2 cross-validation: 0.768\n",
      "Average accuracy for result_data_48 after 5:2 cross-validation: 0.964\n",
      "Average accuracy for result_data_49 after 5:2 cross-validation: 0.97\n",
      "Average accuracy for result_data_50 after 5:2 cross-validation: 0.957\n",
      "Average accuracy for result_data_51 after 5:2 cross-validation: 0.922\n",
      "Average accuracy for result_data_52 after 5:2 cross-validation: 0.921\n",
      "Average accuracy for result_data_53 after 5:2 cross-validation: 0.876\n",
      "Average accuracy for result_data_54 after 5:2 cross-validation: 0.965\n",
      "Average accuracy for result_data_55 after 5:2 cross-validation: 0.963\n",
      "Average accuracy for result_data_56 after 5:2 cross-validation: 0.94\n",
      "Average accuracy for result_data_57 after 5:2 cross-validation: 0.943\n",
      "Average accuracy for result_data_58 after 5:2 cross-validation: 0.944\n",
      "Average accuracy for result_data_59 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_60 after 5:2 cross-validation: 0.97\n",
      "Average accuracy for result_data_61 after 5:2 cross-validation: 0.979\n",
      "Average accuracy for result_data_62 after 5:2 cross-validation: 0.926\n",
      "Average accuracy for result_data_63 after 5:2 cross-validation: 0.979\n",
      "Average accuracy for result_data_64 after 5:2 cross-validation: 0.968\n",
      "Average accuracy for result_data_65 after 5:2 cross-validation: 0.916\n",
      "Average accuracy for result_data_66 after 5:2 cross-validation: 0.965\n",
      "Average accuracy for result_data_67 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_68 after 5:2 cross-validation: 0.985\n",
      "Average accuracy for result_data_69 after 5:2 cross-validation: 0.983\n",
      "Average accuracy for result_data_70 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_71 after 5:2 cross-validation: 0.974\n",
      "Average accuracy for result_data_72 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_73 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_74 after 5:2 cross-validation: 0.925\n",
      "Average accuracy for result_data_75 after 5:2 cross-validation: 0.959\n",
      "Average accuracy for result_data_76 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_77 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_78 after 5:2 cross-validation: 0.91\n",
      "Average accuracy for result_data_79 after 5:2 cross-validation: 0.97\n",
      "Average accuracy for result_data_80 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_81 after 5:2 cross-validation: 0.96\n",
      "Average accuracy for result_data_82 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_83 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_84 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_85 after 5:2 cross-validation: 0.947\n",
      "Average accuracy for result_data_86 after 5:2 cross-validation: 0.894\n",
      "Average accuracy for result_data_87 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_88 after 5:2 cross-validation: 0.982\n",
      "Average accuracy for result_data_89 after 5:2 cross-validation: 0.985\n",
      "Average accuracy for result_data_90 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_91 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_92 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_93 after 5:2 cross-validation: 0.985\n",
      "Average accuracy for result_data_94 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_95 after 5:2 cross-validation: 0.875\n",
      "Average accuracy for result_data_96 after 5:2 cross-validation: 0.983\n",
      "Average accuracy for result_data_97 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_98 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_99 after 5:2 cross-validation: 0.961\n",
      "Average accuracy for result_data_100 after 5:2 cross-validation: 0.984\n",
      "Average accuracy for result_data_101 after 5:2 cross-validation: 0.983\n",
      "Average accuracy for result_data_102 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_103 after 5:2 cross-validation: 0.968\n",
      "Average accuracy for result_data_104 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_105 after 5:2 cross-validation: 0.973\n",
      "Average accuracy for result_data_106 after 5:2 cross-validation: 0.957\n",
      "Average accuracy for result_data_107 after 5:2 cross-validation: 0.985\n",
      "Average accuracy for result_data_108 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_109 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_110 after 5:2 cross-validation: 0.976\n",
      "Average accuracy for result_data_111 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_112 after 5:2 cross-validation: 0.979\n",
      "Average accuracy for result_data_113 after 5:2 cross-validation: 0.974\n",
      "Average accuracy for result_data_114 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_115 after 5:2 cross-validation: 0.974\n",
      "Average accuracy for result_data_116 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_117 after 5:2 cross-validation: 0.987\n",
      "Average accuracy for result_data_118 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_119 after 5:2 cross-validation: 0.979\n",
      "Average accuracy for result_data_120 after 5:2 cross-validation: 0.955\n",
      "Average accuracy for result_data_121 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_122 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_123 after 5:2 cross-validation: 0.983\n",
      "Average accuracy for result_data_124 after 5:2 cross-validation: 0.962\n",
      "Average accuracy for result_data_125 after 5:2 cross-validation: 0.972\n",
      "Average accuracy for result_data_126 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_127 after 5:2 cross-validation: 0.977\n",
      "Average accuracy for result_data_128 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_129 after 5:2 cross-validation: 0.967\n",
      "Average accuracy for result_data_130 after 5:2 cross-validation: 0.982\n",
      "Average accuracy for result_data_131 after 5:2 cross-validation: 0.95\n",
      "Average accuracy for result_data_132 after 5:2 cross-validation: 0.982\n",
      "Average accuracy for result_data_133 after 5:2 cross-validation: 0.971\n",
      "Average accuracy for result_data_134 after 5:2 cross-validation: 0.981\n",
      "Average accuracy for result_data_135 after 5:2 cross-validation: 0.978\n",
      "Average accuracy for result_data_136 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_137 after 5:2 cross-validation: 0.98\n",
      "Average accuracy for result_data_138 after 5:2 cross-validation: 0.983\n"
     ]
    }
   ],
   "source": [
    "#KFold cross validation techquine - 5:2 fold model \n",
    "\n",
    "for i in range(1, 139):\n",
    "    X = result_data_dict[f'result_data_{i}'].iloc[:, :-1]  # Features\n",
    "    y = result_data_dict[f'result_data_{i}'].iloc[:, -1]  # Target\n",
    "\n",
    "    for _ in range(2):\n",
    "        kf_outer = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "        accuracies = []\n",
    "\n",
    "        for train_index, test_index in kf_outer.split(X):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            kf_inner = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "            fold_accuracies = [] \n",
    "\n",
    "            # Inner loop\n",
    "            for inner_train_index, inner_test_index in kf_inner.split(X_train):\n",
    "                X_inner_train, X_inner_test = X_train.iloc[inner_train_index], X_train.iloc[inner_test_index]\n",
    "                y_inner_train, y_inner_test = y_train.iloc[inner_train_index], y_train.iloc[inner_test_index]\n",
    "\n",
    "                # Model\n",
    "                logistic_regression = LogisticRegression(max_iter=1000)\n",
    "\n",
    "                logistic_regression.fit(X_inner_train, y_inner_train)\n",
    "\n",
    "                predictions = logistic_regression.predict(X_inner_test)\n",
    "\n",
    "                # fold accuracy\n",
    "                fold_accuracy = accuracy_score(y_inner_test, predictions)\n",
    "                fold_accuracies.append(fold_accuracy)\n",
    "\n",
    "            #inner loop\n",
    "            average_fold_accuracy = sum(fold_accuracies) / len(fold_accuracies)\n",
    "            accuracies.append(average_fold_accuracy)\n",
    "\n",
    "    # outer loop\n",
    "    average_accuracy2 = sum(accuracies) / len(accuracies)\n",
    "    round_accuracy2 = round(average_accuracy2,3)\n",
    "    print(f\"Average accuracy for result_data_{i} after 5:2 cross-validation: {round_accuracy2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c8aa15ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistics : ROC AUC = 0.740\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#implementation of ROC curve \n",
    "ns_probs = [0 for _ in range(len(y_test))]\n",
    "#predict probabilities\n",
    "lr_probs = model.predict_proba(X_test)\n",
    "#keeping probabilities for positive outcomes only\n",
    "lr_probs= lr_probs[:,1]\n",
    "# predict class values\n",
    "class_val = model.predict(X_test)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_probs)\n",
    "lr_f1, lr_auc = f1_score(y_test, class_val), auc(lr_recall, lr_precision)\n",
    "#calculate scores\n",
    "lr_auc = roc_auc_score(y_test, lr_probs)\n",
    "#auc score \n",
    "print('Logistics : ROC AUC = %.3f' % (lr_auc))\n",
    "#calculate roc curves\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test,lr_probs)\n",
    "#plot the roc curve for the model\n",
    "plt.plot(lr_fpr, lr_tpr, marker = '.', label = 'Logistic')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
